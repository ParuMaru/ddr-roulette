import os
import json
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait # è¿½åŠ 
from selenium.webdriver.support import expected_conditions as EC # è¿½åŠ 
from bs4 import BeautifulSoup
import time
import csv
import re
import unicodedata

# GitHub Secrets
COOKIES_JSON = os.environ.get("DDR_COOKIES")

# ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
FILE_WIKI = "DDR18_songs.csv"
FILE_SCORE = "my_ddr_data.csv"
FILE_CALORIE = "my_calorie_data.csv"
FILE_REVENGE = "lv18_revenge.csv"
FILE_UNPLAYED = "lv18_unplayed.csv"

def create_fingerprint(text):
    if pd.isna(text): return ""
    text = str(text)
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'\((é¬¼|æ¿€|è¸Š|æ¥½|ç¿’)\)$', '', text)
    text = re.sub(r'[^a-zA-Z0-9\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', '', text)
    return text.lower()

def get_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

def update_wiki():
    print("ğŸš€ Wikiæ›´æ–°...")
    driver = get_driver()
    try:
        driver.set_page_load_timeout(60)
        driver.get("https://w.atwiki.jp/asigami/pages/19.html")
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        with open(FILE_WIKI, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ¥½æ›²ãƒ‡ãƒ¼ã‚¿"])
            main = soup.find('div', id='wikibody')
            if main:
                for row in main.find_all('tr'):
                    cells = row.find_all('td')
                    if not cells: continue
                    link = cells[0].find('a')
                    if link: writer.writerow([link.text.strip()])
        print("âœ… Wikiå®Œäº†")
    except Exception as e:
        print(f"âš ï¸ Wikiæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()

def update_official():
    print("ğŸš€ å…¬å¼æ›´æ–°...")
    driver = get_driver()
    URL_SCORE = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/music_data_single.html?offset=0&filter=2&filtertype=18&display=score"
    URL_WORKOUT = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/workout.html"
    
    try:
        driver.set_page_load_timeout(60)
        
        # 1. Cookieã‚»ãƒƒãƒˆ
        driver.get("https://p.eagate.573.jp/")
        if COOKIES_JSON:
            cookies = json.loads(COOKIES_JSON)
            for cookie in cookies:
                if "p.eagate.573.jp" in cookie.get("domain", ""):
                    cd = {
                        "name": cookie["name"],
                        "value": cookie["value"],
                        "domain": cookie["domain"],
                        "path": cookie["path"]
                    }
                    if "sameSite" in cookie:
                        ss = cookie["sameSite"]
                        if ss in ["no_restriction", "None", "none"]: cd["sameSite"] = "None"
                        elif ss in ["lax", "Lax"]: cd["sameSite"] = "Lax"
                        elif ss in ["strict", "Strict"]: cd["sameSite"] = "Strict"
                    if "secure" in cookie: cd["secure"] = cookie["secure"]
                    driver.add_cookie(cd)
        else:
            print("âŒ Cookieãªã—")
            return

        # 2. ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
        driver.get(URL_SCORE)
        
        # â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šURLãƒã‚§ãƒƒã‚¯ã ã‘ã§ãªãã€å®Ÿéš›ã«ã€Œãƒ‡ãƒ¼ã‚¿ãŒå‡ºã‚‹ã¾ã§ã€å¾…ã¤
        print("â³ ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤ºã‚’å¾…æ©Ÿä¸­...")
        try:
            # class="data" ãŒå‡ºã‚‹ã¾ã§æœ€å¤§20ç§’å¾…ã¤
            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, "data")))
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«æ¤œå‡ºï¼")
        except:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆã®èª¿æŸ»ãƒ­ã‚°
            print("âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼šãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            print(f"   ç¾åœ¨ã®URL: {driver.current_url}")
            print(f"   ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {driver.title}")
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«é£›ã°ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
            if "login" in driver.current_url:
                print("   -> ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚CookieãŒç„¡åŠ¹ã§ã™ã€‚")
            return

        # 3. ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹
        score_data = []
        page = 1
        MAX_PAGES = 5

        while page <= MAX_PAGES:
            print(f"  Page {page} èª­ã¿è¾¼ã¿...")
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.find_all('tr', class_='data')
            
            if not rows:
                print("  ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆçµ‚äº†ï¼‰")
                break
            
            current_page_songs = 0
            for row in rows:
                title_div = row.find('div', class_='music_tit')
                name = title_div.text.strip() if title_div else row.find('a').text.strip()
                def check(did):
                    td = row.find('td', id=did)
                    if not td or not td.find('img'): return "æœªãƒ—ãƒ¬ã‚¤"
                    return "æœªã‚¯ãƒªã‚¢(E)" if 'rank_s_e' in td.find('img').get('src', '') else "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                score_data.append([name, check('expert'), check('challenge')])
                current_page_songs += 1
            
            print(f"  -> {current_page_songs}æ›²å–å¾—")

            try:
                # æ¬¡ã¸ãƒœã‚¿ãƒ³åˆ¤å®šã‚’å¼·åŒ–ï¼ˆã‚¯ãƒªãƒƒã‚¯å¯èƒ½ã«ãªã‚‹ã¾ã§å¾…ã¤ï¼‰
                next_div = driver.find_element(By.ID, "next")
                nxt = next_div.find_element(By.TAG_NAME, "a")
                href = nxt.get_attribute("href")
                
                if not href or "javascript:void(0)" in href:
                    print("  æ¬¡ã¸ãƒœã‚¿ãƒ³ãªã—ï¼ˆæ­£å¸¸çµ‚äº†ï¼‰")
                    break
                
                # ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€æ¬¡ã®ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ãŒå‡ºã‚‹ã¾ã§å¾…ã¤
                driver.execute_script("arguments[0].click();", nxt)
                time.sleep(3)
                WebDriverWait(driver, 10).until(EC.staleness_of(rows[0])) # å¤ã„ãƒ‡ãƒ¼ã‚¿ãŒæ¶ˆãˆã‚‹ã®ã‚’å¾…ã¤
                page += 1
            except:
                print("  æ¬¡ã¸ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€ã¾ãŸã¯æœ€çµ‚ãƒšãƒ¼ã‚¸ã®ãŸã‚çµ‚äº†")
                break
        else:
            print("âš ï¸ ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã™ãã‚‹ãŸã‚å¼·åˆ¶çµ‚äº†ã—ã¾ã—ãŸ")

        with open(FILE_SCORE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ›²å", "EXPERTåˆ¤å®š", "CHALLENGEåˆ¤å®š"])
            writer.writerows(score_data)

        # ã‚«ãƒ­ãƒªãƒ¼
        print("ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼å–å¾—...")
        driver.get(URL_WORKOUT)
        # ã‚«ãƒ­ãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå‡ºã‚‹ã¾ã§å¾…ã¤
        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "work_out_left")))
        except:
            print("âš ï¸ ã‚«ãƒ­ãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tbl = soup.find('table', id='work_out_left')
        cal_data = []
        if tbl:
            for row in tbl.find_all('tr'):
                c = row.find_all('td')
                if len(c) >= 4:
                    try:
                        cal_data.append([c[1].text.strip(), c[2].text.strip().replace("æ›²",""), c[3].text.strip().replace("kcal","")])
                    except: continue

        with open(FILE_CALORIE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ—¥ä»˜", "æ›²æ•°", "æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼"])
            writer.writerows(cal_data)
        
        print("âœ… å…¬å¼å®Œäº†")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()

def analyze():
    print("ğŸš€ åˆ†æ...")
    if not os.path.exists(FILE_SCORE) or not os.path.exists(FILE_WIKI): return
    df_wiki = pd.read_csv(FILE_WIKI)
    df_my = pd.read_csv(FILE_SCORE)
    df_my['fp'] = df_my['æ›²å'].apply(create_fingerprint)
    
    rev, unp = [], []
    for _, row in df_wiki.iterrows():
        raw = str(row[0]).strip()
        key = create_fingerprint(raw)
        mode = "CHALLENGEåˆ¤å®š" if "(é¬¼)" in raw else ("EXPERTåˆ¤å®š" if "(æ¿€)" in raw else "BOTH")
        target = df_my[df_my['fp'] == key]
        status = "æœªãƒ—ãƒ¬ã‚¤"
        if not target.empty:
            r = target.iloc[0]
            if mode == "BOTH":
                e, c = str(r.get("EXPERTåˆ¤å®š","")), str(r.get("CHALLENGEåˆ¤å®š",""))
                if "æœªã‚¯ãƒªã‚¢" in e or "æœªã‚¯ãƒªã‚¢" in c: status = "æœªã‚¯ãƒªã‚¢"
                elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" in e and "ã‚¯ãƒªã‚¢æ¸ˆã¿" in c: status = "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                elif "æœªã‚¯ãƒªã‚¢" in e: status = "æœªã‚¯ãƒªã‚¢"
            else: status = str(r.get(mode, ""))
        if "æœªã‚¯ãƒªã‚¢" in status: rev.append(raw)
        elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" not in status: unp.append(raw)
        
    if rev: pd.DataFrame(rev, columns=["æ›²å"]).to_csv(FILE_REVENGE, index=False)
    if unp: pd.DataFrame(unp, columns=["æœªãƒ—ãƒ¬ã‚¤æ›²å"]).to_csv(FILE_UNPLAYED, index=False)
    print("âœ… åˆ†æå®Œäº†")

if __name__ == "__main__":
    update_wiki()
    update_official()
    analyze()