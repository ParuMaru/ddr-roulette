import os
import json
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import csv
import re
import unicodedata
import sys

# GitHub Secrets
COOKIES_JSON = os.environ.get("DDR_COOKIES")

# ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
FILE_WIKI = "DDR18_songs.csv"
FILE_SCORE = "my_ddr_data.csv"
FILE_CALORIE = "my_calorie_data.csv"
FILE_REVENGE = "lv18_revenge.csv"
FILE_UNPLAYED = "lv18_unplayed.csv"

def create_fingerprint(text):
    if pd.isna(text): return ""
    text = str(text)
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'\((é¬¼|æ¿€|è¸Š|æ¥½|ç¿’)\)$', '', text)
    text = re.sub(r'[^a-zA-Z0-9\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', '', text)
    return text.lower()

def get_driver():
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

def update_wiki():
    print("ğŸš€ Wikiæ›´æ–°...")
    driver = get_driver()
    try:
        driver.get("https://w.atwiki.jp/asigami/pages/19.html")
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        temp_data = []
        main = soup.find('div', id='wikibody')
        if main:
            for row in main.find_all('tr'):
                cells = row.find_all('td')
                if not cells: continue
                link = cells[0].find('a')
                if link: temp_data.append([link.text.strip()])
        
        if len(temp_data) > 0:
            with open(FILE_WIKI, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["æ¥½æ›²ãƒ‡ãƒ¼ã‚¿"])
                writer.writerows(temp_data)
            print(f"âœ… Wikiå®Œäº†: {len(temp_data)}æ›²")
        else:
            print("âš ï¸ Wikiãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

    except Exception as e:
        print(f"âš ï¸ Wikiã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()

def update_official():
    print("ğŸš€ å…¬å¼æ›´æ–°...")
    driver = get_driver()
    URL_SCORE = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/music_data_single.html?offset=0&filter=2&filtertype=18&display=score"
    URL_WORKOUT = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/workout.html"
    
    try:
        # Cookieã‚»ãƒƒãƒˆ
        driver.get("https://p.eagate.573.jp/")
        if COOKIES_JSON:
            cookies = json.loads(COOKIES_JSON)
            for cookie in cookies:
                if "p.eagate.573.jp" in cookie.get("domain", ""):
                    cd = {
                        "name": cookie["name"], "value": cookie["value"],
                        "domain": cookie["domain"], "path": cookie["path"]
                    }
                    if "sameSite" in cookie:
                        ss = cookie["sameSite"]
                        if ss in ["no_restriction", "None", "none"]: cd["sameSite"] = "None"
                        elif ss in ["lax", "Lax"]: cd["sameSite"] = "Lax"
                        elif ss in ["strict", "Strict"]: cd["sameSite"] = "Strict"
                    if "secure" in cookie: cd["secure"] = cookie["secure"]
                    driver.add_cookie(cd)
        
        # ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã¸
        driver.get(URL_SCORE)
        
        print("â³ èª­ã¿è¾¼ã¿å¾…æ©Ÿä¸­...")
        try:
            # ãƒ‡ãƒ¼ã‚¿ãŒå‡ºã‚‹ã¾ã§æœ€å¤§20ç§’å¾…ã¤
            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, "data")))
        except:
            print(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (URL: {driver.current_url})")
            if "login" in driver.current_url:
                print("âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã§ã™ã€‚Cookieã®æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return

        score_data = []
        page = 1
        MAX_PAGES = 5

        while page <= MAX_PAGES:
            print(f"  Page {page}...")
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.find_all('tr', class_='data')
            
            if not rows: break
            
            for row in rows:
                title_div = row.find('div', class_='music_tit')
                name = title_div.text.strip() if title_div else row.find('a').text.strip()
                def check(did):
                    td = row.find('td', id=did)
                    if not td or not td.find('img'): return "æœªãƒ—ãƒ¬ã‚¤"
                    return "æœªã‚¯ãƒªã‚¢(E)" if 'rank_s_e' in td.find('img').get('src', '') else "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                score_data.append([name, check('expert'), check('challenge')])
            
            try:
                next_div = driver.find_element(By.ID, "next")
                nxt = next_div.find_element(By.TAG_NAME, "a")
                if not nxt.get_attribute("href") or "javascript:void(0)" in nxt.get_attribute("href"):
                    break
                driver.execute_script("arguments[0].click();", nxt)
                time.sleep(3)
                WebDriverWait(driver, 10).until(EC.staleness_of(rows[0]))
                page += 1
            except:
                break
        
        # â˜…ã“ã“ãŒé‡è¦ï¼š0ä»¶ãªã‚‰ä¿å­˜ã—ãªã„ï¼
        if len(score_data) > 0:
            with open(FILE_SCORE, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["æ›²å", "EXPERTåˆ¤å®š", "CHALLENGEåˆ¤å®š"])
                writer.writerows(score_data)
            print(f"âœ… ã‚¹ã‚³ã‚¢ä¿å­˜å®Œäº†: {len(score_data)}æ›²")
        else:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")

        # ã‚«ãƒ­ãƒªãƒ¼
        print("ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼å–å¾—...")
        driver.get(URL_WORKOUT)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tbl = soup.find('table', id='work_out_left')
        cal_data = []
        if tbl:
            for row in tbl.find_all('tr'):
                c = row.find_all('td')
                if len(c) >= 4:
                    try:
                        cal_data.append([c[1].text.strip(), c[2].text.strip().replace("æ›²",""), c[3].text.strip().replace("kcal","")])
                    except: continue

        if len(cal_data) > 0:
            with open(FILE_CALORIE, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(["æ—¥ä»˜", "æ›²æ•°", "æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼"])
                writer.writerows(cal_data)
            print("âœ… ã‚«ãƒ­ãƒªãƒ¼ä¿å­˜å®Œäº†")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()

def analyze():
    # ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°åˆ†æã‚‚ã—ãªã„
    if not os.path.exists(FILE_SCORE): return

    print("ğŸš€ åˆ†æ...")
    try:
        df_wiki = pd.read_csv(FILE_WIKI)
        df_my = pd.read_csv(FILE_SCORE)
    except:
        return

    df_my['fp'] = df_my['æ›²å'].apply(create_fingerprint)
    
    rev, unp = [], []
    for _, row in df_wiki.iterrows():
        raw = str(row[0]).strip()
        key = create_fingerprint(raw)
        mode = "CHALLENGEåˆ¤å®š" if "(é¬¼)" in raw else ("EXPERTåˆ¤å®š" if "(æ¿€)" in raw else "BOTH")
        target = df_my[df_my['fp'] == key]
        status = "æœªãƒ—ãƒ¬ã‚¤"
        if not target.empty:
            r = target.iloc[0]
            if mode == "BOTH":
                e, c = str(r.get("EXPERTåˆ¤å®š","")), str(r.get("CHALLENGEåˆ¤å®š",""))
                if "æœªã‚¯ãƒªã‚¢" in e or "æœªã‚¯ãƒªã‚¢" in c: status = "æœªã‚¯ãƒªã‚¢"
                elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" in e and "ã‚¯ãƒªã‚¢æ¸ˆã¿" in c: status = "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                elif "æœªã‚¯ãƒªã‚¢" in e: status = "æœªã‚¯ãƒªã‚¢"
            else: status = str(r.get(mode, ""))
        if "æœªã‚¯ãƒªã‚¢" in status: rev.append(raw)
        elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" not in status: unp.append(raw)
        
    if rev: pd.DataFrame(rev, columns=["èª²é¡Œæ›²å"]).to_csv(FILE_REVENGE, index=False)
    if unp: pd.DataFrame(unp, columns=["æœªãƒ—ãƒ¬ã‚¤æ›²å"]).to_csv(FILE_UNPLAYED, index=False)
    print("âœ… åˆ†æå®Œäº†")

if __name__ == "__main__":
    update_wiki()
    update_official()
    analyze()