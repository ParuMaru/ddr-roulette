import os
import json
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import csv
import re
import unicodedata

# GitHubã«é ã‘ãŸCookieã‚’èª­ã¿è¾¼ã‚€
COOKIES_JSON = os.environ.get("DDR_COOKIES")

# ãƒ•ã‚¡ã‚¤ãƒ«åè¨­å®š
FILE_WIKI = "DDR18_songs.csv"
FILE_SCORE = "my_ddr_data.csv"
FILE_CALORIE = "my_calorie_data.csv"
FILE_REVENGE = "lv18_revenge.csv"
FILE_UNPLAYED = "lv18_unplayed.csv"

# --- å…±é€šé–¢æ•° ---
def create_fingerprint(text):
    if pd.isna(text): return ""
    text = str(text)
    text = unicodedata.normalize('NFKC', text)
    text = re.sub(r'\((é¬¼|æ¿€|è¸Š|æ¥½|ç¿’)\)$', '', text)
    text = re.sub(r'[^a-zA-Z0-9\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', '', text)
    return text.lower()

def get_driver():
    options = Options()
    options.add_argument('--headless') # ç”»é¢ãªã—ãƒ¢ãƒ¼ãƒ‰
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

# --- 1. Wikiæ›´æ–° ---
def update_wiki():
    print("ğŸš€ Wikiãƒ‡ãƒ¼ã‚¿æ›´æ–°é–‹å§‹...")
    driver = get_driver()
    try:
        driver.get("https://w.atwiki.jp/asigami/pages/19.html")
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        with open(FILE_WIKI, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ¥½æ›²ãƒ‡ãƒ¼ã‚¿"])
            main_content = soup.find('div', id='wikibody')
            if main_content:
                for row in main_content.find_all('tr'):
                    cells = row.find_all('td')
                    if not cells: continue
                    link = cells[0].find('a')
                    if link:
                        writer.writerow([link.text.strip()])
        print("âœ… Wikiæ›´æ–°å®Œäº†")
    finally:
        driver.quit()

# --- 2. å…¬å¼ãƒ‡ãƒ¼ã‚¿æ›´æ–° (Cookieç‰ˆ) ---
def update_official():
    print("ğŸš€ å…¬å¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°é–‹å§‹...")
    driver = get_driver()
    
    URL_SCORE = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/music_data_single.html?offset=0&filter=2&filtertype=18&display=score"
    URL_WORKOUT = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/workout.html"
    
    try:
        # 1. ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦Cookieã‚’ã‚»ãƒƒãƒˆ
        driver.get("https://p.eagate.573.jp/")
        
        if COOKIES_JSON:
            cookies = json.loads(COOKIES_JSON)
            for cookie in cookies:
                if "p.eagate.573.jp" in cookie.get("domain", ""):
                    cookie_dict = {
                        "name": cookie["name"],
                        "value": cookie["value"],
                        "domain": cookie["domain"],
                        "path": cookie["path"]
                    }
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢é€£ã®ã‚­ãƒ¼ã‚’é™¤å¤–
                    if "sameSite" in cookie: cookie_dict["sameSite"] = cookie["sameSite"]
                    driver.add_cookie(cookie_dict)
        else:
            print("âŒ CookieãŒã‚ã‚Šã¾ã›ã‚“ï¼")
            return

        # 2. ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã¸
        driver.get(URL_SCORE)
        time.sleep(3)
        
        if "login" in driver.current_url:
            print("ğŸ’€ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—ï¼ˆCookieåˆ‡ã‚Œã®å¯èƒ½æ€§ã‚ã‚Šï¼‰")
            return
        
        print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã€‚åé›†é–‹å§‹...")

        # ã‚¹ã‚³ã‚¢åé›†
        score_data = []
        page = 1
        while True:
            print(f"  Page {page}...")
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            rows = soup.find_all('tr', class_='data')
            if not rows: break
            
            for row in rows:
                title_div = row.find('div', class_='music_tit')
                name = title_div.text.strip() if title_div else row.find('a').text.strip()
                
                def check(did):
                    td = row.find('td', id=did)
                    if not td: return "ãƒ‡ãƒ¼ã‚¿ãªã—"
                    if not td.find('img'): return "æœªãƒ—ãƒ¬ã‚¤"
                    return "æœªã‚¯ãƒªã‚¢(E)" if 'rank_s_e' in td.find('img').get('src', '') else "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                
                score_data.append([name, check('expert'), check('challenge')])
            
            # æ¬¡ã¸
            try:
                nxt = driver.find_element(By.ID, "next").find_element(By.TAG_NAME, "a")
                if "javascript:void(0)" in nxt.get_attribute("href"): break
                driver.execute_script("arguments[0].click();", nxt)
                time.sleep(3)
                page += 1
            except:
                break

        with open(FILE_SCORE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ›²å", "EXPERTåˆ¤å®š", "CHALLENGEåˆ¤å®š"])
            writer.writerows(score_data)

        # ã‚«ãƒ­ãƒªãƒ¼åé›†
        driver.get(URL_WORKOUT)
        time.sleep(3)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        tbl = soup.find('table', id='work_out_left')
        cal_data = []
        if tbl:
            for row in tbl.find_all('tr'):
                c = row.find_all('td')
                if len(c) >= 4:
                    try:
                        cal_data.append([c[1].text.strip(), c[2].text.strip().replace("æ›²",""), c[3].text.strip().replace("kcal","")])
                    except: continue

        with open(FILE_CALORIE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ—¥ä»˜", "æ›²æ•°", "æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼"])
            writer.writerows(cal_data)

        print("âœ… å…¬å¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°å®Œäº†")

    finally:
        driver.quit()

# --- 3. åˆ†æ ---
def analyze():
    print("ğŸš€ ãƒ‡ãƒ¼ã‚¿åˆ†æé–‹å§‹...")
    if not os.path.exists(FILE_SCORE) or not os.path.exists(FILE_WIKI): return

    df_wiki = pd.read_csv(FILE_WIKI)
    df_my = pd.read_csv(FILE_SCORE)
    
    # ç…§åˆ
    df_my['fp'] = df_my['æ›²å'].apply(create_fingerprint)
    
    rev, unp = [], []
    for _, row in df_wiki.iterrows():
        raw = str(row[0]).strip()
        key = create_fingerprint(raw)
        
        mode = "BOTH"
        if "(é¬¼)" in raw: mode = "CHALLENGEåˆ¤å®š"
        elif "(æ¿€)" in raw: mode = "EXPERTåˆ¤å®š"
        
        target = df_my[df_my['fp'] == key]
        status = "æœªãƒ—ãƒ¬ã‚¤"
        
        if not target.empty:
            row_data = target.iloc[0]
            if mode == "BOTH":
                e, c = str(row_data.get("EXPERTåˆ¤å®š","")), str(row_data.get("CHALLENGEåˆ¤å®š",""))
                if "æœªã‚¯ãƒªã‚¢" in e or "æœªã‚¯ãƒªã‚¢" in c: status = "æœªã‚¯ãƒªã‚¢"
                elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" in e and "ã‚¯ãƒªã‚¢æ¸ˆã¿" in c: status = "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                elif "æœªã‚¯ãƒªã‚¢" in e: status = "æœªã‚¯ãƒªã‚¢"
            else:
                status = str(row_data.get(mode, ""))
        
        if "æœªã‚¯ãƒªã‚¢" in status: rev.append(raw)
        elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" not in status: unp.append(raw)
        
    if rev: pd.DataFrame(rev, columns=["æ›²å"]).to_csv(FILE_REVENGE, index=False)
    if unp: pd.DataFrame(unp, columns=["æœªãƒ—ãƒ¬ã‚¤æ›²å"]).to_csv(FILE_UNPLAYED, index=False)
    print("âœ… åˆ†æå®Œäº†")

if __name__ == "__main__":
    update_wiki()
    update_official()
    analyze()