import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import os
import csv
import re
import unicodedata

# --- è¨­å®šï¼šä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å ---
# ãƒ•ã‚¡ã‚¤ãƒ«åã¯å…ƒã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æŒ‡å®šã«åˆã‚ã›ã¤ã¤ã€é€£æºã—ã‚„ã™ã„ã‚ˆã†ã«å®šç¾©
base_dir = os.path.dirname(os.path.abspath(__file__))
FILE_WIKI = os.path.join(base_dir, "DDR18_songs.csv")
FILE_SCORE = os.path.join(base_dir, "my_ddr_data.csv")
FILE_CALORIE = os.path.join(base_dir, "my_calorie_data.csv")
FILE_REVENGE = os.path.join(base_dir, "lv18_revenge.csv")
FILE_UNPLAYED = os.path.join(base_dir, "lv18_unplayed.csv")

# ==========================================
# å…±é€šé–¢æ•°: æ–‡å­—åˆ—æ­£è¦åŒ– (extract_lv18_separate.pyã‚ˆã‚Š)
# ==========================================
def create_fingerprint(text):
    if pd.isna(text): return ""
    text = str(text)
    # 1. NFKCæ­£è¦åŒ–
    text = unicodedata.normalize('NFKC', text)
    # 2. é›£æ˜“åº¦è¡¨è¨˜ (é¬¼)(æ¿€) ãªã©ã‚’å‰Šé™¤
    text = re.sub(r'\((é¬¼|æ¿€|è¸Š|æ¥½|ç¿’)\)$', '', text)
    # 3. è‹±æ•°å­—ã¨æ—¥æœ¬èªä»¥å¤–ã‚’å‰Šé™¤
    text = re.sub(r'[^a-zA-Z0-9\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', '', text)
    # 4. å°æ–‡å­—åŒ–
    return text.lower()


# ==========================================
# æ©Ÿèƒ½1: Wikiãƒ‡ãƒ¼ã‚¿ã®æ›´æ–° (scrapping_wiki_data.pyãƒ™ãƒ¼ã‚¹)
# ==========================================
def update_wiki_data():
    """Wikiã‹ã‚‰Lv18ã®æ¥½æ›²ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¦ä¿å­˜ã™ã‚‹"""
    print("ğŸš€ Wikiãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãŸURL
    url = "https://w.atwiki.jp/asigami/pages/19.html"

    options = webdriver.ChromeOptions()
    # options.add_argument('--headless') # ç”»é¢ã‚’è¡¨ç¤ºã—ãªã„å ´åˆã¯æœ‰åŠ¹åŒ–
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {url}")
        driver.get(url)

        # èª­ã¿è¾¼ã¿å¾…ã¡ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰é€šã‚Š5ç§’ï¼‰
        print("â³ èª­ã¿è¾¼ã¿å¾…ã¡ï¼ˆ5ç§’ï¼‰...")
        time.sleep(5)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        with open(FILE_WIKI, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ¥½æ›²ãƒ‡ãƒ¼ã‚¿"]) # å…ƒã‚³ãƒ¼ãƒ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            
            # æœ¬æ–‡ã‚¨ãƒªã‚¢(wikibody)ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            main_content = soup.find('div', id='wikibody')
            if not main_content:
                return "ã‚¨ãƒ©ãƒ¼: Wikiã®æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

            count = 0
            for row in main_content.find_all('tr'):
                cells = row.find_all('td')
                if not cells: continue

                target_cell = cells[0]
                link_tag = target_cell.find('a')

                if link_tag:
                    song_name = link_tag.text.strip()
                    writer.writerow([song_name])
                    count += 1
        
        return f"æˆåŠŸ: Wikiãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ ({count}æ›²)"

    except Exception as e:
        return f"Wikiæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"

    finally:
        driver.quit()


# ==========================================
# æ©Ÿèƒ½2: å…¬å¼ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–° (scrape_official_ddr.pyãƒ™ãƒ¼ã‚¹)
# ==========================================
def update_official_data():
    """å…¬å¼ã‹ã‚‰ã‚¹ã‚³ã‚¢ã¨ã‚«ãƒ­ãƒªãƒ¼ã‚’å–å¾—ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ç¶­æŒãƒ»å…¨ãƒšãƒ¼ã‚¸å–å¾—ï¼‰"""
    print("ğŸš€ å…¬å¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãŸURL (display=score)
    URL_SCORE = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/music_data_single.html?offset=0&filter=2&filtertype=18&display=score"
    URL_WORKOUT = "https://p.eagate.573.jp/game/ddr/ddrworld/playdata/workout.html"

    options = webdriver.ChromeOptions()
    
    # â˜…ãƒ­ã‚°ã‚¤ãƒ³ç¶­æŒã®ãŸã‚ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
    profile_path = os.path.join(os.getcwd(), "ddr_profile")
    options.add_argument(f'--user-data-dir={profile_path}')
    
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    try:
        # 1. ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã¸ç§»å‹• & ãƒ­ã‚°ã‚¤ãƒ³å¾…æ©Ÿå‡¦ç†
        driver.get(URL_SCORE)
        
        print("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèªä¸­...")
        # ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾…ã¡ãƒ«ãƒ¼ãƒ—
        for i in range(60):
            current_url = driver.current_url
            
            # ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«åˆ¥ãƒšãƒ¼ã‚¸ï¼ˆãƒˆãƒƒãƒ—ãªã©ï¼‰ã«é£›ã°ã•ã‚ŒãŸå ´åˆã€ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã«æˆ»ã™
            if "login" not in current_url and "eagate.573.jp" in current_url:
                if "music_data_single" not in current_url:
                    print("ğŸ”„ ã‚¹ã‚³ã‚¢ãƒšãƒ¼ã‚¸ã¸å†ç§»å‹•ã—ã¾ã™...")
                    driver.get(URL_SCORE)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«(class="data")ãŒè¦‹ã¤ã‹ã‚Œã°OK
            try:
                WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.CLASS_NAME, "data")))
                print("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚åé›†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
                break 
            except:
                time.sleep(1)
        else:
            return "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ãƒ­ã‚°ã‚¤ãƒ³ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºã‚’ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

        # --- ã‚¹ã‚³ã‚¢åé›† (scrape_official_ddr.pyã®ãƒ­ã‚¸ãƒƒã‚¯) ---
        print("ğŸ’¿ ã‚¹ã‚³ã‚¢åé›†ä¸­...")
        
        with open(FILE_SCORE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ›²å", "EXPERTåˆ¤å®š", "CHALLENGEåˆ¤å®š"]) # å…ƒã‚³ãƒ¼ãƒ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼

            total_songs = 0
            page_num = 1
            
            while True:
                print(f"  - Page {page_num}...")
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                rows = soup.find_all('tr', class_='data')

                if not rows:
                    print("  ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚³ã‚¢åé›†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break

                for row in rows:
                    title_div = row.find('div', class_='music_tit')
                    song_name = title_div.text.strip() if title_div else row.find('a').text.strip()

                    def check_status(diff_id):
                        td = row.find('td', id=diff_id)
                        if not td: return "ãƒ‡ãƒ¼ã‚¿ãªã—"
                        img = td.find('img')
                        if not img: return "æœªãƒ—ãƒ¬ã‚¤"
                        src = img.get('src', '')
                        # display=scoreã®å ´åˆã¯ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã§åˆ¤å®š
                        return "æœªã‚¯ãƒªã‚¢(E)" if 'rank_s_e' in src else "ã‚¯ãƒªã‚¢æ¸ˆã¿"

                    exp = check_status('expert')
                    cha = check_status('challenge')
                    
                    writer.writerow([song_name, exp, cha])
                    total_songs += 1

                # æ¬¡ã¸ãƒœã‚¿ãƒ³å‡¦ç†
                try:
                    next_div = driver.find_element(By.ID, "next")
                    next_link = next_div.find_element(By.TAG_NAME, "a")
                    href = next_link.get_attribute("href")
                    
                    if not href or "javascript:void(0)" in href:
                        break 

                    driver.execute_script("arguments[0].click();", next_link)
                    time.sleep(3) 
                    page_num += 1
                except:
                    break 
        
        print(f"âœ… ã‚¹ã‚³ã‚¢å–å¾—å®Œäº†: {total_songs}æ›²")


        # --- ã‚«ãƒ­ãƒªãƒ¼å–å¾— (scrape_official_ddr.pyã®ãƒ­ã‚¸ãƒƒã‚¯) ---
        print("ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼åé›†ä¸­...")
        driver.get(URL_WORKOUT)
        time.sleep(3)
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        table = soup.find('table', id='work_out_left')
        
        calorie_data = []
        if table:
            for row in table.find_all('tr'):
                cells = row.find_all('td')
                if len(cells) >= 4:
                    try:
                        date_t = cells[1].text.strip()
                        count_t = cells[2].text.strip().replace("æ›²", "").strip()
                        cal_t = cells[3].text.strip().replace("kcal", "").strip()
                        if date_t and cal_t:
                            calorie_data.append([date_t, count_t, cal_t])
                    except:
                        continue
        
        with open(FILE_CALORIE, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["æ—¥ä»˜", "æ›²æ•°", "æ¶ˆè²»ã‚«ãƒ­ãƒªãƒ¼"]) # å…ƒã‚³ãƒ¼ãƒ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerows(calorie_data)

        return f"æˆåŠŸ: ã‚¹ã‚³ã‚¢({total_songs}ä»¶)ã¨ã‚«ãƒ­ãƒªãƒ¼ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼"

    except Exception as e:
        return f"å…¬å¼æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}"
    finally:
        driver.quit()


# ==========================================
# æ©Ÿèƒ½3: ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»æŠ½å‡º (extract_lv18_separate.pyãƒ™ãƒ¼ã‚¹)
# ==========================================
def analyze_data():
    """Wikiã¨å…¬å¼ãƒ‡ãƒ¼ã‚¿ã‚’çªãåˆã‚ã›ã¦ãƒªã‚¹ãƒˆã‚’ä½œã‚‹"""
    try:
        if not os.path.exists(FILE_SCORE) or not os.path.exists(FILE_WIKI):
            return "ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«Wikiã¨å…¬å¼ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚"
            
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df_wiki = pd.read_csv(FILE_WIKI)
        df_my = pd.read_csv(FILE_SCORE)
        
        # åˆ—åç‰¹å®šï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        wiki_col = df_wiki.columns[0] # "æ¥½æ›²ãƒ‡ãƒ¼ã‚¿"
        my_col = "æ›²å" if "æ›²å" in df_my.columns else df_my.columns[0] # "æ›²å"

        # ç…§åˆç”¨ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ—ãƒªãƒ³ãƒˆä½œæˆ
        df_my['fingerprint'] = df_my[my_col].apply(create_fingerprint)

        revenge_list = []
        unplayed_list = []

        # å…¨æ›²ãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
        for index, row in df_wiki.iterrows():
            raw_name = str(row[wiki_col]).strip()
            search_key = create_fingerprint(raw_name)
            
            # é›£æ˜“åº¦åˆ¤å®š (é¬¼/æ¿€)
            target_mode = "BOTH"
            if "(é¬¼)" in raw_name: target_mode = "CHALLENGEåˆ¤å®š"
            elif "(æ¿€)" in raw_name: target_mode = "EXPERTåˆ¤å®š"

            # ç…§åˆ
            user_row = df_my[df_my['fingerprint'] == search_key]
            
            status = "æœªãƒ—ãƒ¬ã‚¤"
            
            if not user_row.empty:
                if target_mode == "BOTH":
                    e = str(user_row.iloc[0].get("EXPERTåˆ¤å®š", ""))
                    c = str(user_row.iloc[0].get("CHALLENGEåˆ¤å®š", ""))
                    if "æœªã‚¯ãƒªã‚¢" in e or "æœªã‚¯ãƒªã‚¢" in c: 
                        status = "æœªã‚¯ãƒªã‚¢"
                    elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" in e and "ã‚¯ãƒªã‚¢æ¸ˆã¿" in c:
                        status = "ã‚¯ãƒªã‚¢æ¸ˆã¿"
                    else:
                        if "æœªã‚¯ãƒªã‚¢" in e: status = "æœªã‚¯ãƒªã‚¢"
                else:
                    val = user_row.iloc[0].get(target_mode, "")
                    if pd.notna(val):
                        status = str(val)

            # åˆ†åˆ¥
            if "æœªã‚¯ãƒªã‚¢" in status:
                revenge_list.append(raw_name)
            elif "ã‚¯ãƒªã‚¢æ¸ˆã¿" in status:
                continue
            else:
                unplayed_list.append(raw_name)

        # ä¿å­˜
        if revenge_list:
            pd.DataFrame(revenge_list, columns=["æ›²å"]).to_csv(FILE_REVENGE, index=False, encoding='utf-8_sig')
        
        if unplayed_list:
            pd.DataFrame(unplayed_list, columns=["æœªãƒ—ãƒ¬ã‚¤æ›²å"]).to_csv(FILE_UNPLAYED, index=False, encoding='utf-8_sig')
        
        return "æˆåŠŸ: ãƒªãƒ™ãƒ³ã‚¸ãƒªã‚¹ãƒˆã¨æœªãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸï¼"
        
    except Exception as e:
        return f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}"